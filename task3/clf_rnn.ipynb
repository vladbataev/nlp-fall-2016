{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/ubuntu/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/ubuntu/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting nltk==3.2\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning.\n",
      "  SNIMissingWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "  Downloading nltk-3.2.tar.gz (1.2MB)\n",
      "\u001b[K    100% |################################| 1.2MB 1.1MB/s \n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "  Running setup.py install for nltk ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
      "\u001b[?25hSuccessfully installed nltk-3.2\n"
     ]
    }
   ],
   "source": [
    "! sudo pip install --upgrade nltk==3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.is_blocked.mean()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print \"Blocked ratio\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.5\n",
      "Count: 549992\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "import random\n",
    "\n",
    "num_of_blocked_samples = df.is_blocked.sum()\n",
    "not_blocked_df = df[df.is_blocked == 0]\n",
    "not_blocked_indexes = set(not_blocked_df.index) \n",
    "downsample_indexes = set()\n",
    "\n",
    "while len(downsample_indexes) < num_of_blocked_samples:\n",
    "    index = random.randint(0, len(not_blocked_df) - 1)\n",
    "    if index not in downsample_indexes:\n",
    "        downsample_indexes.add(index)\n",
    "    \n",
    "df = pd.concat([df[df.is_blocked == 1], not_blocked_df.iloc[list(downsample_indexes)]])\n",
    "\n",
    "print \"Blocked ratio:\", df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFkCAYAAAAKf8APAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDCiSJKk6BhRJklSd/w92wY4PE/8tjAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbda8ff2d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = [pair[0] for pair in [(token, occur) for (token, occur) in token_counts.items() if occur >= min_count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 87826\n",
      "Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) < 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 15)\n",
      "Поездки на таможню, печать в паспорте -> [43277 14694 55331 82030 80142 17356     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [ 8352     0 30472     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [28848 23438     0  3638 33981     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "categories = [{\"category\": sample[0], \"subcategory\": sample[1]} for sample in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Split into training and test set.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(\n",
    "    title_tokens, desc_tokens, df_non_text, target)\n",
    "data_tuple = title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = False #save\n",
    "read_prepared_data = True #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"готово\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print \"done\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nontext_tr = nontext_tr.values\n",
    "nontext_ts = nontext_ts.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id) + 1, output_size=300)\n",
    "descr_nn = lasagne.layers.GRULayer(descr_nn, num_units=512)\n",
    "descr_nn = lasagne.layers.GRULayer(descr_nn, num_units=512)\n",
    "descr_nn = lasagne.layers.flatten(descr_nn)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id) + 1, output_size=64)\n",
    "title_nn = lasagne.layers.GRULayer(title_nn, num_units=128)\n",
    "title_nn = lasagne.layers.GRULayer(title_nn, num_units=128)\n",
    "title_nn = lasagne.layers.flatten(title_nn)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=256)\n",
    "cat_nn = lasagne.layers.DropoutLayer(cat_nn, p=0.5)\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.ConcatLayer([descr_nn, title_nn, cat_nn])                                \n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, 512)\n",
    "nn = lasagne.layers.DropoutLayer(nn, p=0.5)\n",
    "nn = lasagne.layers.DenseLayer(nn, 1, nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction, target_y, delta=1.0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adam(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn, deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(prediction, target_y, delta=1.0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\", 100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        \n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      "Train:\n",
      "\tloss: 2924.04626502\n",
      "\tacc: 0.70504950495\n",
      "\tauc: 0.735519672844\n",
      "\tap@k: 0.158319988635\n",
      "Val:\n",
      "\tloss: 7424.88709957\n",
      "\tacc: 0.747425742574\n",
      "\tauc: 0.782356515874\n",
      "\tap@k: 0.931569104105\n",
      "Work time 68.2657101154\n",
      "Epoch number 1\n",
      "Train:\n",
      "\tloss: 2740.1963859\n",
      "\tacc: 0.770396039604\n",
      "\tauc: 0.786803092065\n",
      "\tap@k: 0.0946112565704\n",
      "Val:\n",
      "\tloss: 2353.30699176\n",
      "\tacc: 0.795346534653\n",
      "\tauc: 0.803518844304\n",
      "\tap@k: 0.950556243755\n",
      "Work time 68.1147208214\n",
      "Epoch number 2\n",
      "Train:\n",
      "\tloss: 2432.00662736\n",
      "\tacc: 0.786435643564\n",
      "\tauc: 0.80121458171\n",
      "\tap@k: 0.171131194238\n",
      "Val:\n",
      "\tloss: 325.505945642\n",
      "\tacc: 0.787425742574\n",
      "\tauc: 0.827257554407\n",
      "\tap@k: 0.377660943463\n",
      "Work time 67.8178021908\n",
      "Epoch number 3\n",
      "Train:\n",
      "\tloss: 82.5427772679\n",
      "\tacc: 0.776435643564\n",
      "\tauc: 0.852604434865\n",
      "\tap@k: 0.346601737444\n",
      "Val:\n",
      "\tloss: 537.103738846\n",
      "\tacc: 0.691881188119\n",
      "\tauc: 0.839502403424\n",
      "\tap@k: 0.723170153299\n",
      "Work time 67.7043008804\n",
      "Epoch number 4\n",
      "Train:\n",
      "\tloss: 205.866526548\n",
      "\tacc: 0.751287128713\n",
      "\tauc: 0.866880539383\n",
      "\tap@k: 0.435837314818\n",
      "Val:\n",
      "\tloss: 111.019070307\n",
      "\tacc: 0.706732673267\n",
      "\tauc: 0.884258992533\n",
      "\tap@k: 0.761073992006\n",
      "Work time 67.6654338837\n",
      "Epoch number 5\n",
      "Train:\n",
      "\tloss: 65.1956410028\n",
      "\tacc: 0.755445544554\n",
      "\tauc: 0.885870824952\n",
      "\tap@k: 0.582928750314\n",
      "Val:\n",
      "\tloss: 108.024448523\n",
      "\tacc: 0.74198019802\n",
      "\tauc: 0.857897758892\n",
      "\tap@k: 0.715715710128\n",
      "Work time 67.874710083\n",
      "Epoch number 6\n",
      "Train:\n",
      "\tloss: 45.5672905733\n",
      "\tacc: 0.753366336634\n",
      "\tauc: 0.872620890439\n",
      "\tap@k: 0.445378857553\n",
      "Val:\n",
      "\tloss: 126.088339394\n",
      "\tacc: 0.749504950495\n",
      "\tauc: 0.853099384769\n",
      "\tap@k: 0.637185515095\n",
      "Work time 68.0956702232\n",
      "Epoch number 7\n",
      "Train:\n",
      "\tloss: 10.8935994102\n",
      "\tacc: 0.786732673267\n",
      "\tauc: 0.888972345453\n",
      "\tap@k: 0.725731883473\n",
      "Val:\n",
      "\tloss: 43.4787727277\n",
      "\tacc: 0.814356435644\n",
      "\tauc: 0.891630148282\n",
      "\tap@k: 0.949856283222\n",
      "Work time 68.2854039669\n",
      "Epoch number 8\n",
      "Train:\n",
      "\tloss: 9.02593622043\n",
      "\tacc: 0.817425742574\n",
      "\tauc: 0.89537408684\n",
      "\tap@k: 0.714882925848\n",
      "Val:\n",
      "\tloss: 45.701846502\n",
      "\tacc: 0.849900990099\n",
      "\tauc: 0.907807929826\n",
      "\tap@k: 0.897650451484\n",
      "Work time 68.4327089787\n",
      "Epoch number 9\n",
      "Train:\n",
      "\tloss: 21.5529798214\n",
      "\tacc: 0.818514851485\n",
      "\tauc: 0.885588831268\n",
      "\tap@k: 0.368661355053\n",
      "Val:\n",
      "\tloss: 141.818858654\n",
      "\tacc: 0.796732673267\n",
      "\tauc: 0.850739866576\n",
      "\tap@k: 0.99188108719\n",
      "Work time 68.4070119858\n",
      "Epoch number 10\n",
      "Train:\n",
      "\tloss: 22.8082018289\n",
      "\tacc: 0.826930693069\n",
      "\tauc: 0.904072392832\n",
      "\tap@k: 0.744175446505\n",
      "Val:\n",
      "\tloss: 38.8099243922\n",
      "\tacc: 0.852277227723\n",
      "\tauc: 0.912044899253\n",
      "\tap@k: 0.800801983851\n",
      "Work time 68.3808429241\n",
      "Epoch number 11\n",
      "Train:\n",
      "\tloss: 39.9358999942\n",
      "\tacc: 0.79504950495\n",
      "\tauc: 0.870161642601\n",
      "\tap@k: 0.573128786705\n",
      "Val:\n",
      "\tloss: 24.4932179633\n",
      "\tacc: 0.837128712871\n",
      "\tauc: 0.925063297157\n",
      "\tap@k: 0.876082768865\n",
      "Work time 68.5291688442\n",
      "Epoch number 12\n",
      "Train:\n",
      "\tloss: 57.3362351793\n",
      "\tacc: 0.784752475248\n",
      "\tauc: 0.893999456826\n",
      "\tap@k: 0.821609243165\n",
      "Val:\n",
      "\tloss: 16.0884329825\n",
      "\tacc: 0.862079207921\n",
      "\tauc: 0.941777017182\n",
      "\tap@k: 0.933457726603\n",
      "Work time 68.4311449528\n",
      "Epoch number 13\n",
      "Train:\n",
      "\tloss: 8.41556527621\n",
      "\tacc: 0.790792079208\n",
      "\tauc: 0.893533556722\n",
      "\tap@k: 0.903276098899\n",
      "Val:\n",
      "\tloss: 0.616006336233\n",
      "\tacc: 0.887623762376\n",
      "\tauc: 0.955728584497\n",
      "\tap@k: 0.948881995281\n",
      "Work time 68.5370540619\n",
      "Epoch number 14\n",
      "Train:\n",
      "\tloss: 0.494224771826\n",
      "\tacc: 0.802475247525\n",
      "\tauc: 0.903612558652\n",
      "\tap@k: 0.997626308242\n",
      "Val:\n",
      "\tloss: 2.06091046447\n",
      "\tacc: 0.900792079208\n",
      "\tauc: 0.963588461457\n",
      "\tap@k: 0.964177261083\n",
      "Work time 68.3830170631\n",
      "Epoch number 15\n",
      "Train:\n",
      "\tloss: 0.462421015944\n",
      "\tacc: 0.807524752475\n",
      "\tauc: 0.902777658506\n",
      "\tap@k: 0.996191982081\n",
      "Val:\n",
      "\tloss: 0.544566070641\n",
      "\tacc: 0.898118811881\n",
      "\tauc: 0.962964715887\n",
      "\tap@k: 0.971095420216\n",
      "Work time 68.7334039211\n",
      "Epoch number 16\n",
      "Train:\n",
      "\tloss: 0.458544042199\n",
      "\tacc: 0.804851485149\n",
      "\tauc: 0.908222777961\n",
      "\tap@k: 0.929515830113\n",
      "Val:\n",
      "\tloss: 0.45739931287\n",
      "\tacc: 0.908910891089\n",
      "\tauc: 0.969314347357\n",
      "\tap@k: 0.952024741467\n",
      "Work time 68.432970047\n",
      "Epoch number 17\n",
      "Train:\n",
      "\tloss: 0.65697390374\n",
      "\tacc: 0.788316831683\n",
      "\tauc: 0.898714625396\n",
      "\tap@k: 0.754922631032\n",
      "Val:\n",
      "\tloss: 0.575828569052\n",
      "\tacc: 0.903663366337\n",
      "\tauc: 0.963997174963\n",
      "\tap@k: 0.952820613524\n",
      "Work time 68.4108998775\n",
      "Epoch number 18\n",
      "Train:\n",
      "\tloss: 0.460675943436\n",
      "\tacc: 0.810396039604\n",
      "\tauc: 0.920673896147\n",
      "\tap@k: 0.88903653061\n",
      "Val:\n",
      "\tloss: 0.434495048455\n",
      "\tacc: 0.916237623762\n",
      "\tauc: 0.97036632589\n",
      "\tap@k: 1.0\n",
      "Work time 68.4697189331\n",
      "Epoch number 19\n",
      "Train:\n",
      "\tloss: 0.406863900119\n",
      "\tacc: 0.81495049505\n",
      "\tauc: 0.926948057741\n",
      "\tap@k: 0.920007840979\n",
      "Val:\n",
      "\tloss: 0.409941417897\n",
      "\tacc: 0.918118811881\n",
      "\tauc: 0.971383526937\n",
      "\tap@k: 1.0\n",
      "Work time 68.5802190304\n",
      "Epoch number 20\n",
      "Train:\n",
      "\tloss: 0.506117195721\n",
      "\tacc: 0.810396039604\n",
      "\tauc: 0.924470485158\n",
      "\tap@k: 0.868255578514\n",
      "Val:\n",
      "\tloss: 0.417823592025\n",
      "\tacc: 0.920594059406\n",
      "\tauc: 0.975251995118\n",
      "\tap@k: 1.0\n",
      "Work time 68.4864139557\n",
      "Epoch number 21\n",
      "Train:\n",
      "\tloss: 0.401283389014\n",
      "\tacc: 0.813663366337\n",
      "\tauc: 0.928045592398\n",
      "\tap@k: 0.997125491443\n",
      "Val:\n",
      "\tloss: 0.410673623722\n",
      "\tacc: 0.926534653465\n",
      "\tauc: 0.97675908144\n",
      "\tap@k: 0.999033303107\n",
      "Work time 68.4731030464\n",
      "Epoch number 22\n",
      "Train:\n",
      "\tloss: 0.381593689956\n",
      "\tacc: 0.820792079208\n",
      "\tauc: 0.934490457085\n",
      "\tap@k: 0.974057557836\n",
      "Val:\n",
      "\tloss: 0.398138551785\n",
      "\tacc: 0.918712871287\n",
      "\tauc: 0.975059084649\n",
      "\tap@k: 0.990551493363\n",
      "Work time 68.3815448284\n",
      "Epoch number 23\n",
      "Train:\n",
      "\tloss: 0.368468404058\n",
      "\tacc: 0.826633663366\n",
      "\tauc: 0.940054976605\n",
      "\tap@k: 0.989434673464\n",
      "Val:\n",
      "\tloss: 0.377983268859\n",
      "\tacc: 0.928811881188\n",
      "\tauc: 0.978606098533\n",
      "\tap@k: 1.0\n",
      "Work time 68.507694006\n",
      "Epoch number 24\n",
      "Train:\n",
      "\tloss: 0.373780456022\n",
      "\tacc: 0.820198019802\n",
      "\tauc: 0.940932767825\n",
      "\tap@k: 0.998786829904\n",
      "Val:\n",
      "\tloss: 0.378362220825\n",
      "\tacc: 0.927722772277\n",
      "\tauc: 0.978896674698\n",
      "\tap@k: 1.0\n",
      "Work time 68.7842171192\n",
      "Epoch number 25\n",
      "Train:\n",
      "\tloss: 0.389285910364\n",
      "\tacc: 0.820099009901\n",
      "\tauc: 0.940439572721\n",
      "\tap@k: 0.934173874982\n",
      "Val:\n",
      "\tloss: 0.398079054206\n",
      "\tacc: 0.921386138614\n",
      "\tauc: 0.976799080107\n",
      "\tap@k: 0.98965426064\n",
      "Work time 68.3991379738\n",
      "Epoch number 26\n",
      "Train:\n",
      "\tloss: 0.425010844954\n",
      "\tacc: 0.835346534653\n",
      "\tauc: 0.93141825088\n",
      "\tap@k: 0.856347640341\n",
      "Val:\n",
      "\tloss: 0.392205547078\n",
      "\tacc: 0.924356435644\n",
      "\tauc: 0.980498828121\n",
      "\tap@k: 1.0\n",
      "Work time 68.4045269489\n",
      "Epoch number 27\n",
      "Train:\n",
      "\tloss: 0.354840516964\n",
      "\tacc: 0.86495049505\n",
      "\tauc: 0.946413665956\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.403397285482\n",
      "\tacc: 0.926732673267\n",
      "\tauc: 0.977610142932\n",
      "\tap@k: 1.0\n",
      "Work time 68.4504368305\n",
      "Epoch number 28\n",
      "Train:\n",
      "\tloss: 0.383978648171\n",
      "\tacc: 0.86495049505\n",
      "\tauc: 0.942372176273\n",
      "\tap@k: 0.930438384398\n",
      "Val:\n",
      "\tloss: 0.405603094608\n",
      "\tacc: 0.93396039604\n",
      "\tauc: 0.979529259991\n",
      "\tap@k: 0.997435654337\n",
      "Work time 68.6107461452\n",
      "Epoch number 29\n",
      "Train:\n",
      "\tloss: 0.348109004743\n",
      "\tacc: 0.866336633663\n",
      "\tauc: 0.946965461899\n",
      "\tap@k: 0.995849632417\n",
      "Val:\n",
      "\tloss: 0.386200908473\n",
      "\tacc: 0.933168316832\n",
      "\tauc: 0.980162820496\n",
      "\tap@k: 1.0\n",
      "Work time 68.4712629318\n",
      "Epoch number 30\n",
      "Train:\n",
      "\tloss: 0.34239165895\n",
      "\tacc: 0.867821782178\n",
      "\tauc: 0.947803093516\n",
      "\tap@k: 0.981768457959\n",
      "Val:\n",
      "\tloss: 0.360741400947\n",
      "\tacc: 0.929306930693\n",
      "\tauc: 0.982886554543\n",
      "\tap@k: 1.0\n",
      "Work time 68.3833670616\n",
      "Epoch number 31\n",
      "Train:\n",
      "\tloss: 0.346705551711\n",
      "\tacc: 0.871485148515\n",
      "\tauc: 0.948878327219\n",
      "\tap@k: 0.974792445284\n",
      "Val:\n",
      "\tloss: 0.340188805181\n",
      "\tacc: 0.937326732673\n",
      "\tauc: 0.983052216981\n",
      "\tap@k: 1.0\n",
      "Work time 68.1716330051\n",
      "Epoch number 32\n",
      "Train:\n",
      "\tloss: 0.317284254871\n",
      "\tacc: 0.871287128713\n",
      "\tauc: 0.95243722456\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.360250756741\n",
      "\tacc: 0.930693069307\n",
      "\tauc: 0.980171515287\n",
      "\tap@k: 0.998657579388\n",
      "Work time 68.3559799194\n",
      "Epoch number 33\n",
      "Train:\n",
      "\tloss: 0.359064230852\n",
      "\tacc: 0.857425742574\n",
      "\tauc: 0.944944978551\n",
      "\tap@k: 0.99804260955\n",
      "Val:\n",
      "\tloss: 0.353935713538\n",
      "\tacc: 0.936534653465\n",
      "\tauc: 0.980808389196\n",
      "\tap@k: 1.0\n",
      "Work time 68.4053390026\n",
      "Epoch number 34\n",
      "Train:\n",
      "\tloss: 0.321089795337\n",
      "\tacc: 0.872376237624\n",
      "\tauc: 0.953698380696\n",
      "\tap@k: 0.988105259059\n",
      "Val:\n",
      "\tloss: 0.325298989278\n",
      "\tacc: 0.940396039604\n",
      "\tauc: 0.983775378458\n",
      "\tap@k: 0.999189473985\n",
      "Work time 68.4316911697\n",
      "Epoch number 35\n",
      "Train:\n",
      "\tloss: 0.294628513668\n",
      "\tacc: 0.888514851485\n",
      "\tauc: 0.956329666519\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.325201855407\n",
      "\tacc: 0.940396039604\n",
      "\tauc: 0.981780554946\n",
      "\tap@k: 1.0\n",
      "Work time 68.709182024\n",
      "Epoch number 36\n",
      "Train:\n",
      "\tloss: 0.31142315551\n",
      "\tacc: 0.887524752475\n",
      "\tauc: 0.956092343888\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.308053774547\n",
      "\tacc: 0.925643564356\n",
      "\tauc: 0.982825387208\n",
      "\tap@k: 1.0\n",
      "Work time 68.378911972\n",
      "Epoch number 37\n",
      "Train:\n",
      "\tloss: 0.279971284633\n",
      "\tacc: 0.896633663366\n",
      "\tauc: 0.961710365135\n",
      "\tap@k: 0.988588033328\n",
      "Val:\n",
      "\tloss: 0.293743289777\n",
      "\tacc: 0.929108910891\n",
      "\tauc: 0.983710730981\n",
      "\tap@k: 1.0\n",
      "Work time 68.1461188793\n",
      "Epoch number 38\n",
      "Train:\n",
      "\tloss: 0.266549129046\n",
      "\tacc: 0.902277227723\n",
      "\tauc: 0.962427973005\n",
      "\tap@k: 0.960899146435\n",
      "Val:\n",
      "\tloss: 0.326580454985\n",
      "\tacc: 0.925643564356\n",
      "\tauc: 0.983121097314\n",
      "\tap@k: 1.0\n",
      "Work time 68.1834280491\n",
      "Epoch number 39\n",
      "Train:\n",
      "\tloss: 0.290722501552\n",
      "\tacc: 0.887227722772\n",
      "\tauc: 0.951622047187\n",
      "\tap@k: 0.997886254452\n",
      "Val:\n",
      "\tloss: 0.318474797786\n",
      "\tacc: 0.914851485149\n",
      "\tauc: 0.980634184995\n",
      "\tap@k: 0.997142825757\n",
      "Work time 68.2043690681\n",
      "Epoch number 40\n",
      "Train:\n",
      "\tloss: 0.259402226452\n",
      "\tacc: 0.895742574257\n",
      "\tauc: 0.961470355512\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.397824576848\n",
      "\tacc: 0.937821782178\n",
      "\tauc: 0.981197012932\n",
      "\tap@k: 0.984054726901\n",
      "Work time 68.1972520351\n",
      "Epoch number 41\n",
      "Train:\n",
      "\tloss: 0.248905157226\n",
      "\tacc: 0.90297029703\n",
      "\tauc: 0.961897312936\n",
      "\tap@k: 0.998786829904\n",
      "Val:\n",
      "\tloss: 0.310002980161\n",
      "\tacc: 0.912277227723\n",
      "\tauc: 0.980751020982\n",
      "\tap@k: 1.0\n",
      "Work time 68.2850430012\n",
      "Epoch number 42\n",
      "Train:\n",
      "\tloss: 0.267874814841\n",
      "\tacc: 0.892574257426\n",
      "\tauc: 0.956720945552\n",
      "\tap@k: 0.936203753737\n",
      "Val:\n",
      "\tloss: 0.285950660151\n",
      "\tacc: 0.934356435644\n",
      "\tauc: 0.981533540008\n",
      "\tap@k: 1.0\n",
      "Work time 68.0997138023\n",
      "Epoch number 43\n",
      "Train:\n",
      "\tloss: 0.23572228021\n",
      "\tacc: 0.900891089109\n",
      "\tauc: 0.954133043981\n",
      "\tap@k: 0.995416597402\n",
      "Val:\n",
      "\tloss: 0.276826731114\n",
      "\tacc: 0.937920792079\n",
      "\tauc: 0.98240666586\n",
      "\tap@k: 0.996534981126\n",
      "Work time 68.0520989895\n",
      "Epoch number 44\n",
      "Train:\n",
      "\tloss: 0.222416449325\n",
      "\tacc: 0.910693069307\n",
      "\tauc: 0.95577550364\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.276321914848\n",
      "\tacc: 0.940891089109\n",
      "\tauc: 0.982140128372\n",
      "\tap@k: 0.999840921636\n",
      "Work time 68.1934640408\n",
      "Epoch number 45\n",
      "Train:\n",
      "\tloss: 0.238463841511\n",
      "\tacc: 0.90198019802\n",
      "\tauc: 0.949287456521\n",
      "\tap@k: 0.955631777579\n",
      "Val:\n",
      "\tloss: 0.31122916617\n",
      "\tacc: 0.930099009901\n",
      "\tauc: 0.981246190671\n",
      "\tap@k: 0.997859547903\n",
      "Work time 68.4545328617\n",
      "Epoch number 46\n",
      "Train:\n",
      "\tloss: 0.230612135907\n",
      "\tacc: 0.909504950495\n",
      "\tauc: 0.95140057195\n",
      "\tap@k: 0.995697287467\n",
      "Val:\n",
      "\tloss: 0.305056069369\n",
      "\tacc: 0.940198019802\n",
      "\tauc: 0.979761011949\n",
      "\tap@k: 0.987775878295\n",
      "Work time 68.330024004\n",
      "Epoch number 47\n",
      "Train:\n",
      "\tloss: 0.214663444858\n",
      "\tacc: 0.91198019802\n",
      "\tauc: 0.954714858351\n",
      "\tap@k: 0.995797702541\n",
      "Val:\n",
      "\tloss: 0.288645410431\n",
      "\tacc: 0.928712871287\n",
      "\tauc: 0.977549398965\n",
      "\tap@k: 0.998870707001\n",
      "Work time 68.3568110466\n",
      "Epoch number 48\n",
      "Train:\n",
      "\tloss: 0.219876832378\n",
      "\tacc: 0.912574257426\n",
      "\tauc: 0.950834381545\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.243597257373\n",
      "\tacc: 0.939900990099\n",
      "\tauc: 0.968839423293\n",
      "\tap@k: 1.0\n",
      "Work time 68.2502288818\n",
      "Epoch number 49\n",
      "Train:\n",
      "\tloss: 0.20128875337\n",
      "\tacc: 0.916336633663\n",
      "\tauc: 0.954305366667\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.249058505922\n",
      "\tacc: 0.94504950495\n",
      "\tauc: 0.982996951586\n",
      "\tap@k: 1.0\n",
      "Work time 68.1680510044\n",
      "Epoch number 50\n",
      "Train:\n",
      "\tloss: 0.216256821887\n",
      "\tacc: 0.911782178218\n",
      "\tauc: 0.949297841345\n",
      "\tap@k: 0.999791922287\n",
      "Val:\n",
      "\tloss: 0.259928483987\n",
      "\tacc: 0.931881188119\n",
      "\tauc: 0.977465614299\n",
      "\tap@k: 1.0\n",
      "Work time 68.156692028\n",
      "Epoch number 51\n",
      "Train:\n",
      "\tloss: 0.203409774268\n",
      "\tacc: 0.917227722772\n",
      "\tauc: 0.949713976882\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.256150352748\n",
      "\tacc: 0.939603960396\n",
      "\tauc: 0.979801509461\n",
      "\tap@k: 0.998409051011\n",
      "Work time 67.9715509415\n",
      "Epoch number 52\n",
      "Train:\n",
      "\tloss: 0.225821519323\n",
      "\tacc: 0.905742574257\n",
      "\tauc: 0.945825806545\n",
      "\tap@k: 0.999448751306\n",
      "Val:\n",
      "\tloss: 0.258784925318\n",
      "\tacc: 0.946930693069\n",
      "\tauc: 0.981868434858\n",
      "\tap@k: 0.999376391166\n",
      "Work time 68.2225809097\n",
      "Epoch number 53\n",
      "Train:\n",
      "\tloss: 0.215090635602\n",
      "\tacc: 0.910594059406\n",
      "\tauc: 0.947688788447\n",
      "\tap@k: 0.993851113593\n",
      "Val:\n",
      "\tloss: 0.258917434434\n",
      "\tacc: 0.935940594059\n",
      "\tauc: 0.980228918733\n",
      "\tap@k: 1.0\n",
      "Work time 68.0982379913\n",
      "Epoch number 54\n",
      "Train:\n",
      "\tloss: 0.200231729811\n",
      "\tacc: 0.919504950495\n",
      "\tauc: 0.950162108211\n",
      "\tap@k: 0.996534981126\n",
      "Val:\n",
      "\tloss: 0.26193172204\n",
      "\tacc: 0.935346534653\n",
      "\tauc: 0.974732854962\n",
      "\tap@k: 0.978342891509\n",
      "Work time 68.2291409969\n",
      "Epoch number 55\n",
      "Train:\n",
      "\tloss: 0.196240364325\n",
      "\tacc: 0.919207920792\n",
      "\tauc: 0.948064123818\n",
      "\tap@k: 0.982732511942\n",
      "Val:\n",
      "\tloss: 0.249223089218\n",
      "\tacc: 0.940099009901\n",
      "\tauc: 0.974530307482\n",
      "\tap@k: 0.991362019378\n",
      "Work time 68.0883169174\n",
      "Epoch number 56\n",
      "Train:\n",
      "\tloss: 0.236712352929\n",
      "\tacc: 0.917524752475\n",
      "\tauc: 0.945038950787\n",
      "\tap@k: 0.977611472236\n",
      "Val:\n",
      "\tloss: 0.234409052582\n",
      "\tacc: 0.941386138614\n",
      "\tauc: 0.977744828304\n",
      "\tap@k: 1.0\n",
      "Work time 68.0071549416\n",
      "Epoch number 57\n",
      "Train:\n",
      "\tloss: 0.185887932388\n",
      "\tacc: 0.92198019802\n",
      "\tauc: 0.947715566904\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.227978884318\n",
      "\tacc: 0.947227722772\n",
      "\tauc: 0.981780552057\n",
      "\tap@k: 1.0\n",
      "Work time 68.1530508995\n",
      "Epoch number 58\n",
      "Train:\n",
      "\tloss: 0.193445392331\n",
      "\tacc: 0.919702970297\n",
      "\tauc: 0.947735409216\n",
      "\tap@k: 0.986774805705\n",
      "Val:\n",
      "\tloss: 0.244825259913\n",
      "\tacc: 0.948811881188\n",
      "\tauc: 0.98280706193\n",
      "\tap@k: 0.997778328188\n",
      "Work time 68.1244730949\n",
      "Epoch number 59\n",
      "Train:\n",
      "\tloss: 0.20734088893\n",
      "\tacc: 0.914851485149\n",
      "\tauc: 0.942911012119\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.286123684733\n",
      "\tacc: 0.946732673267\n",
      "\tauc: 0.979958325643\n",
      "\tap@k: 1.0\n",
      "Work time 68.0797569752\n",
      "Epoch number 60\n",
      "Train:\n",
      "\tloss: 0.196061910314\n",
      "\tacc: 0.920495049505\n",
      "\tauc: 0.94951009411\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.249620207106\n",
      "\tacc: 0.944653465347\n",
      "\tauc: 0.979632250415\n",
      "\tap@k: 1.0\n",
      "Work time 68.1398108006\n",
      "Epoch number 61\n",
      "Train:\n",
      "\tloss: 0.185866771683\n",
      "\tacc: 0.921287128713\n",
      "\tauc: 0.947294908916\n",
      "\tap@k: 0.99725249186\n",
      "Val:\n",
      "\tloss: 0.253776166817\n",
      "\tacc: 0.95\n",
      "\tauc: 0.978439349646\n",
      "\tap@k: 0.997570817868\n",
      "Work time 68.2629580498\n",
      "Epoch number 62\n",
      "Train:\n",
      "\tloss: 0.181179012467\n",
      "\tacc: 0.922376237624\n",
      "\tauc: 0.951572316612\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.246052566041\n",
      "\tacc: 0.930297029703\n",
      "\tauc: 0.977139902355\n",
      "\tap@k: 1.0\n",
      "Work time 68.2134308815\n",
      "Epoch number 63\n",
      "Train:\n",
      "\tloss: 0.16227713674\n",
      "\tacc: 0.930198019802\n",
      "\tauc: 0.955934328773\n",
      "\tap@k: 0.999606874898\n",
      "Val:\n",
      "\tloss: 0.25804167512\n",
      "\tacc: 0.950891089109\n",
      "\tauc: 0.983297156465\n",
      "\tap@k: 0.996645807276\n",
      "Work time 68.1804800034\n",
      "Epoch number 64\n",
      "Train:\n",
      "\tloss: 0.203664943706\n",
      "\tacc: 0.919504950495\n",
      "\tauc: 0.949875598923\n",
      "\tap@k: 0.995906261909\n",
      "Val:\n",
      "\tloss: 0.229675136332\n",
      "\tacc: 0.943861386139\n",
      "\tauc: 0.982023471124\n",
      "\tap@k: 0.986898419388\n",
      "Work time 68.1156570911\n",
      "Epoch number 65\n",
      "Train:\n",
      "\tloss: 0.186609258003\n",
      "\tacc: 0.921188118812\n",
      "\tauc: 0.951276980799\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.239091956792\n",
      "\tacc: 0.948514851485\n",
      "\tauc: 0.983138576294\n",
      "\tap@k: 1.0\n",
      "Work time 67.9386050701\n",
      "Epoch number 66\n",
      "Train:\n",
      "\tloss: 0.189903359708\n",
      "\tacc: 0.92198019802\n",
      "\tauc: 0.95325830508\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.210055657419\n",
      "\tacc: 0.954653465347\n",
      "\tauc: 0.984367669141\n",
      "\tap@k: 1.0\n",
      "Work time 67.9541039467\n",
      "Epoch number 67\n",
      "Train:\n",
      "\tloss: 0.178030160547\n",
      "\tacc: 0.924257425743\n",
      "\tauc: 0.94981316711\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.324611394907\n",
      "\tacc: 0.952079207921\n",
      "\tauc: 0.986140919843\n",
      "\tap@k: 1.0\n",
      "Work time 67.9713730812\n",
      "Epoch number 68\n",
      "Train:\n",
      "\tloss: 0.152365723453\n",
      "\tacc: 0.934059405941\n",
      "\tauc: 0.959580732721\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.218597156718\n",
      "\tacc: 0.955148514851\n",
      "\tauc: 0.986749390856\n",
      "\tap@k: 1.0\n",
      "Work time 67.7164487839\n",
      "Epoch number 69\n",
      "Train:\n",
      "\tloss: 0.181415513854\n",
      "\tacc: 0.925643564356\n",
      "\tauc: 0.951946388069\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.217760431501\n",
      "\tacc: 0.944554455446\n",
      "\tauc: 0.980908178493\n",
      "\tap@k: 0.996263622021\n",
      "Work time 67.7529351711\n",
      "Epoch number 70\n",
      "Train:\n",
      "\tloss: 0.166032527349\n",
      "\tacc: 0.928217821782\n",
      "\tauc: 0.953925076322\n",
      "\tap@k: 0.989103011946\n",
      "Val:\n",
      "\tloss: 0.182894259373\n",
      "\tacc: 0.95297029703\n",
      "\tauc: 0.984711283341\n",
      "\tap@k: 1.0\n",
      "Work time 67.8901371956\n",
      "Epoch number 71\n",
      "Train:\n",
      "\tloss: 0.181790677315\n",
      "\tacc: 0.932376237624\n",
      "\tauc: 0.959304569165\n",
      "\tap@k: 0.997405710631\n",
      "Val:\n",
      "\tloss: 0.258208296405\n",
      "\tacc: 0.943465346535\n",
      "\tauc: 0.981060908468\n",
      "\tap@k: 0.999537377498\n",
      "Work time 67.9686682224\n",
      "Epoch number 72\n",
      "Train:\n",
      "\tloss: 0.160196419718\n",
      "\tacc: 0.933069306931\n",
      "\tauc: 0.955003947879\n",
      "\tap@k: 0.999589614771\n",
      "Val:\n",
      "\tloss: 0.205105939566\n",
      "\tacc: 0.952178217822\n",
      "\tauc: 0.982147438641\n",
      "\tap@k: 1.0\n",
      "Work time 68.3611021042\n",
      "Epoch number 73\n",
      "Train:\n",
      "\tloss: 0.154091342192\n",
      "\tacc: 0.937821782178\n",
      "\tauc: 0.96064048203\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.184757992702\n",
      "\tacc: 0.953762376238\n",
      "\tauc: 0.98375552803\n",
      "\tap@k: 0.998168699955\n",
      "Work time 67.743090868\n",
      "Epoch number 74\n",
      "Train:\n",
      "\tloss: 0.149419440385\n",
      "\tacc: 0.939900990099\n",
      "\tauc: 0.965154402077\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.199927825928\n",
      "\tacc: 0.951188118812\n",
      "\tauc: 0.982174071022\n",
      "\tap@k: 1.0\n",
      "Work time 67.7918848991\n",
      "Epoch number 75\n",
      "Train:\n",
      "\tloss: 0.127087624409\n",
      "\tacc: 0.953069306931\n",
      "\tauc: 0.968881663593\n",
      "\tap@k: 0.997351869044\n",
      "Val:\n",
      "\tloss: 0.426399369671\n",
      "\tacc: 0.923168316832\n",
      "\tauc: 0.969622616908\n",
      "\tap@k: 0.990582053468\n",
      "Work time 67.8571341038\n",
      "Epoch number 76\n",
      "Train:\n",
      "\tloss: 0.151194814686\n",
      "\tacc: 0.940891089109\n",
      "\tauc: 0.963257674568\n",
      "\tap@k: 0.997093093335\n",
      "Val:\n",
      "\tloss: 0.212243454698\n",
      "\tacc: 0.933465346535\n",
      "\tauc: 0.974361941176\n",
      "\tap@k: 0.989071819987\n",
      "Work time 68.3212571144\n",
      "Epoch number 77\n",
      "Train:\n",
      "\tloss: 0.143139355242\n",
      "\tacc: 0.942673267327\n",
      "\tauc: 0.972177663761\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.830554513199\n",
      "\tacc: 0.942772277228\n",
      "\tauc: 0.979935532585\n",
      "\tap@k: 0.987620059967\n",
      "Work time 68.4510469437\n",
      "Epoch number 78\n",
      "Train:\n",
      "\tloss: 0.607108319467\n",
      "\tacc: 0.887623762376\n",
      "\tauc: 0.917068740876\n",
      "\tap@k: 0.942981216168\n",
      "Val:\n",
      "\tloss: 0.302468997512\n",
      "\tacc: 0.927425742574\n",
      "\tauc: 0.961351764232\n",
      "\tap@k: 0.956579354901\n",
      "Work time 68.4645719528\n",
      "Epoch number 79\n",
      "Train:\n",
      "\tloss: 0.225681563055\n",
      "\tacc: 0.904455445545\n",
      "\tauc: 0.932580480374\n",
      "\tap@k: 0.84446160304\n",
      "Val:\n",
      "\tloss: 0.23592110458\n",
      "\tacc: 0.929900990099\n",
      "\tauc: 0.963465747052\n",
      "\tap@k: 0.925555776389\n",
      "Work time 68.5139138699\n",
      "Epoch number 80\n",
      "Train:\n",
      "\tloss: 0.204115717435\n",
      "\tacc: 0.910495049505\n",
      "\tauc: 0.93471630358\n",
      "\tap@k: 0.913354369705\n",
      "Val:\n",
      "\tloss: 0.221731509684\n",
      "\tacc: 0.936534653465\n",
      "\tauc: 0.973283460038\n",
      "\tap@k: 0.955813100073\n",
      "Work time 68.2206950188\n",
      "Epoch number 81\n",
      "Train:\n",
      "\tloss: 0.177938311755\n",
      "\tacc: 0.924554455446\n",
      "\tauc: 0.947158163788\n",
      "\tap@k: 0.882497159741\n",
      "Val:\n",
      "\tloss: 0.25463762543\n",
      "\tacc: 0.942079207921\n",
      "\tauc: 0.975539328437\n",
      "\tap@k: 0.923775963518\n",
      "Work time 68.427448988\n",
      "Epoch number 82\n",
      "Train:\n",
      "\tloss: 0.18401126236\n",
      "\tacc: 0.926138613861\n",
      "\tauc: 0.946735312991\n",
      "\tap@k: 0.897724114864\n",
      "Val:\n",
      "\tloss: 0.256199824485\n",
      "\tacc: 0.947128712871\n",
      "\tauc: 0.976189470402\n",
      "\tap@k: 0.97089356982\n",
      "Work time 68.4172759056\n",
      "Epoch number 83\n",
      "Train:\n",
      "\tloss: 0.189499296753\n",
      "\tacc: 0.923465346535\n",
      "\tauc: 0.949181384068\n",
      "\tap@k: 0.909744892041\n",
      "Val:\n",
      "\tloss: 0.22909449143\n",
      "\tacc: 0.937326732673\n",
      "\tauc: 0.966217987923\n",
      "\tap@k: 0.918707088324\n",
      "Work time 68.5367629528\n",
      "Epoch number 84\n",
      "Train:\n",
      "\tloss: 0.159849621901\n",
      "\tacc: 0.933663366337\n",
      "\tauc: 0.952436694253\n",
      "\tap@k: 0.907127239004\n",
      "Val:\n",
      "\tloss: 0.199201757182\n",
      "\tacc: 0.935247524752\n",
      "\tauc: 0.966789632155\n",
      "\tap@k: 0.919517546147\n",
      "Work time 68.4841721058\n",
      "Epoch number 85\n",
      "Train:\n",
      "\tloss: 0.150949450891\n",
      "\tacc: 0.936336633663\n",
      "\tauc: 0.960211059713\n",
      "\tap@k: 0.934807235457\n",
      "Val:\n",
      "\tloss: 0.168750103452\n",
      "\tacc: 0.949603960396\n",
      "\tauc: 0.97624218928\n",
      "\tap@k: 0.982449552115\n",
      "Work time 68.5638651848\n",
      "Epoch number 86\n",
      "Train:\n",
      "\tloss: 0.130809636039\n",
      "\tacc: 0.945346534653\n",
      "\tauc: 0.965045557776\n",
      "\tap@k: 0.939164693347\n",
      "Val:\n",
      "\tloss: 0.182375706375\n",
      "\tacc: 0.952673267327\n",
      "\tauc: 0.976967491981\n",
      "\tap@k: 0.968498720395\n",
      "Work time 68.6436879635\n",
      "Epoch number 87\n",
      "Train:\n",
      "\tloss: 0.130577431002\n",
      "\tacc: 0.946138613861\n",
      "\tauc: 0.962199664908\n",
      "\tap@k: 0.907328149047\n",
      "Val:\n",
      "\tloss: 0.167613426004\n",
      "\tacc: 0.945544554455\n",
      "\tauc: 0.972322915638\n",
      "\tap@k: 0.965234889377\n",
      "Work time 68.4974000454\n",
      "Epoch number 88\n",
      "Train:\n",
      "\tloss: 0.126184500957\n",
      "\tacc: 0.947821782178\n",
      "\tauc: 0.966566468792\n",
      "\tap@k: 0.944495038043\n",
      "Val:\n",
      "\tloss: 0.182296932062\n",
      "\tacc: 0.938316831683\n",
      "\tauc: 0.970479322329\n",
      "\tap@k: 0.967874648619\n",
      "Work time 68.5161941051\n",
      "Epoch number 89\n",
      "Train:\n",
      "\tloss: 0.126902933729\n",
      "\tacc: 0.948316831683\n",
      "\tauc: 0.966288366293\n",
      "\tap@k: 0.944638587932\n",
      "Val:\n",
      "\tloss: 0.163328377982\n",
      "\tacc: 0.94603960396\n",
      "\tauc: 0.972410304571\n",
      "\tap@k: 0.930000261491\n",
      "Work time 68.3842380047\n",
      "Epoch number 90\n",
      "Train:\n",
      "\tloss: 0.128930824334\n",
      "\tacc: 0.948712871287\n",
      "\tauc: 0.966947875544\n",
      "\tap@k: 0.984307447904\n",
      "Val:\n",
      "\tloss: 0.190303409926\n",
      "\tacc: 0.954653465347\n",
      "\tauc: 0.981673165687\n",
      "\tap@k: 0.969726345541\n",
      "Work time 68.2562050819\n",
      "Epoch number 91\n",
      "Train:\n",
      "\tloss: 0.12508516684\n",
      "\tacc: 0.946633663366\n",
      "\tauc: 0.965355308784\n",
      "\tap@k: 0.954499742235\n",
      "Val:\n",
      "\tloss: 0.159459222868\n",
      "\tacc: 0.955346534653\n",
      "\tauc: 0.980260017802\n",
      "\tap@k: 0.969750440171\n",
      "Work time 68.3931322098\n",
      "Epoch number 92\n",
      "Train:\n",
      "\tloss: 0.11616776978\n",
      "\tacc: 0.951782178218\n",
      "\tauc: 0.968534717824\n",
      "\tap@k: 0.934927221363\n",
      "Val:\n",
      "\tloss: 0.165144038938\n",
      "\tacc: 0.958118811881\n",
      "\tauc: 0.983567833485\n",
      "\tap@k: 0.984737399997\n",
      "Work time 68.2879772186\n",
      "Epoch number 93\n",
      "Train:\n",
      "\tloss: 0.126352799859\n",
      "\tacc: 0.947326732673\n",
      "\tauc: 0.962150500015\n",
      "\tap@k: 0.944943656191\n",
      "Val:\n",
      "\tloss: 0.170406679629\n",
      "\tacc: 0.949108910891\n",
      "\tauc: 0.974778157045\n",
      "\tap@k: 0.981613923507\n",
      "Work time 68.4354200363\n",
      "Epoch number 94\n",
      "Train:\n",
      "\tloss: 0.112365049903\n",
      "\tacc: 0.951485148515\n",
      "\tauc: 0.966398238665\n",
      "\tap@k: 0.945107911461\n",
      "Val:\n",
      "\tloss: 0.174105970193\n",
      "\tacc: 0.940198019802\n",
      "\tauc: 0.972333768592\n",
      "\tap@k: 0.977745152095\n",
      "Work time 68.1388700008\n",
      "Epoch number 95\n",
      "Train:\n",
      "\tloss: 0.122962192456\n",
      "\tacc: 0.95\n",
      "\tauc: 0.966270023255\n",
      "\tap@k: 0.978212017734\n",
      "Val:\n",
      "\tloss: 0.159264401864\n",
      "\tacc: 0.953267326733\n",
      "\tauc: 0.974732035279\n",
      "\tap@k: 0.984407871178\n",
      "Work time 68.3026130199\n",
      "Epoch number 96\n",
      "Train:\n",
      "\tloss: 0.123245133344\n",
      "\tacc: 0.947821782178\n",
      "\tauc: 0.964548974323\n",
      "\tap@k: 0.980564237393\n",
      "Val:\n",
      "\tloss: 0.171925382478\n",
      "\tacc: 0.95396039604\n",
      "\tauc: 0.982012119499\n",
      "\tap@k: 0.981862057218\n",
      "Work time 68.1654849052\n",
      "Epoch number 97\n",
      "Train:\n",
      "\tloss: 0.122681880157\n",
      "\tacc: 0.947425742574\n",
      "\tauc: 0.965145913453\n",
      "\tap@k: 0.949375949296\n",
      "Val:\n",
      "\tloss: 0.165416122575\n",
      "\tacc: 0.958811881188\n",
      "\tauc: 0.982185382888\n",
      "\tap@k: 0.984366668815\n",
      "Work time 68.1823179722\n",
      "Epoch number 98\n",
      "Train:\n",
      "\tloss: 0.107993587019\n",
      "\tacc: 0.953564356436\n",
      "\tauc: 0.970047024687\n",
      "\tap@k: 0.948918484821\n",
      "Val:\n",
      "\tloss: 0.189317358715\n",
      "\tacc: 0.950099009901\n",
      "\tauc: 0.973117744933\n",
      "\tap@k: 0.97117026353\n",
      "Work time 67.8363609314\n",
      "Epoch number 99\n",
      "Train:\n",
      "\tloss: 0.123124881682\n",
      "\tacc: 0.951089108911\n",
      "\tauc: 0.964397972217\n",
      "\tap@k: 0.951194799345\n",
      "Val:\n",
      "\tloss: 0.162425233214\n",
      "\tacc: 0.953762376238\n",
      "\tauc: 0.977549297546\n",
      "\tap@k: 0.986570204362\n",
      "Work time 67.7350809574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import time\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    start_time = time.time()\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Epoch number {}\".format(i)\n",
    "    print \"Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    print 'Work time {}'.format(time.time() - start_time)\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \n"
     ]
    }
   ],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.162176783976\n",
      "\tacc: 0.955625909753\n",
      "\tauc: 0.979378256063\n",
      "\tap@k: 0.97074891125\n",
      "\n",
      "AUC:\n",
      "\tОтличное решение! (good)\n",
      "\n",
      "Accuracy:\n",
      "\tОтличный результат! (good)\n",
      "\n",
      "Average precision at K:\n",
      "\tОтличный результат (good)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
